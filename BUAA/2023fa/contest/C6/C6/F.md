# F 2023 Mid - 信息之熵

时间限制：1000ms 内存限制：65536kb

## 题目背景

信息熵（information entropy）是信息论的基本概念。描述信息源各可能事件发生的不确定性。20 世纪 40 年代，香农（C.E.Shannon）借鉴了热力学的概念，把信息中排除了冗余后的平均信息量称为“信息熵”，并给出了计算信息熵的数学表达式。

## 题目描述

假设一个信息源总共可能发出 $n$ 种符号，分别是 $\alpha_1,\alpha_2,\ldots,\alpha_n,$ 它们被发出的概率分别为 $p_1,p_2,\ldots,p_n,$ 那么这个信息源的信息熵 $H$ 为：

$$H=−\Sigma_{i=1}^np_i\log_2(p_i)$$

现有一个信息源在一定时间内发出一些字符，**以频率作为概率**，请你统计一下这个信息源的信息熵。
请注意：**只统计大写字母、小写字母和数字，其余的符号和空白字符等应予以忽略。统计时区分大小写。**

## 输入

输入一些字符，不超过 $10000$ 个。

## 输出

一个实数，保留四位小数，表示仅统计大小写字母和数字时，该信息源的信息熵。

## 输入样例

```text
BUAA accoding
2023.10.26
```

## 输出样例

```text
3.7842
```

## 样例解释

样例中各字符的出现频次统计表如下：

字符|频数|频率
:-:|:-:|:-:
0|2|0.1
1|1|0.05
2|3|0.15
3|1|0.05
6|1|0.05
A|2|0.1
B|1|0.05
U|1|0.05
a|1|0.05
c|2|0.1
d|1|0.05
g|1|0.05
i|1|0.05
n|1|0.05
o|1|0.05

## Hint

可以使用 `math.h` 库中的 `log2(x)` 函数返回 $\log_2(x).$

也可以使用 `log(x)` 函数返回 $\ln(x),$ 利用换底公式计算 $\log_2(x).$

换底公式:
$$\log_ab=\frac{\log_cb}{log_ca}$$
